{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061bac1d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9969eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", quantization_config=bnb_config)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728ae420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"NilanE/ParallelFiction-Ja_En-100k\")\n",
    "split = ds['train'].train_test_split(test_size=0.01, seed=42)\n",
    "ds = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9def84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    genres_list = sample['meta']['novelupdates']['genres']\n",
    "    \n",
    "    # Example: \"Adventure, Fantasy, Shounen\"\n",
    "    genres_str = \", \".join(genres_list)\n",
    "    \n",
    "    # We ask the model to write a story including these specific genres.\n",
    "    instruction = f\"「{genres_str}」のジャンルが含まれるストーリーを書いて。\"\n",
    "    \n",
    "    # Standard Rinna Format: ユーザー: {instruction}\\nシステム: {response}\n",
    "    prompt = f\"ユーザー: {instruction}\\nシステム: {sample['src']}\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply to the dataset\n",
    "formatted_ds = ds.map(format_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e457437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ユーザー: 「Romance, Slice of Life」のジャンルが含まれるストーリーを書いて。\\nシステム: 真面目な話をこれからすると思うと、自然と真面目な顔つきになる。\\n姫川もさっきから、俺の事を直視しながら、どう切り出すか考えているのだろう。\\n部屋には時計の秒針がコチコチ音を鳴らすだけで、他の音は聞こえなかった。\\n静寂の中、ついに意を決し姫川は語り始める。\\n「実は......」\\n『ぐぅぅぅ~~~』\\nこれは、姫川の作戦か? 俺は思わず頬を吊り上げ、にやけてしまった。\\n「ひ、姫川。今の音は?」\\n姫川は頬を赤くしながら下を向いてしまい、返事はない。\\n少し肩が震えているので、恐らく恥ずかしさを我慢しているのだろう。\\n「い、今の音は何でもありません。き、気にしないでください」\\n「ちなみに、姫川が最後に食べたのはいつだ?」\\n今だに頬を赤くしている姫川は、いつもより若干幼く見える。\\n同い年だし、普段は俺よりもよっぽど大人っぽい仕草を見せられているが、今は後輩のように見える。\\n「昨夜食べたっきりで、今日はまだ何も......」\\n「そっか。話は後だ、ちょっと待ってろ」\\n俺は、姫川をリビングに放置し台所に向かう。\\n台所から手に持ったタウン誌を姫川に放り投げる。反射的に受け取った姫川はタウン誌片手に俺の方を見る。\\n「今月のタウン誌。少し時間かかるからそれでも読んで暇つぶしててくれ。あ、そこの陰にコンセントあるから充電とか適当にしてていいぞ」\\nそして俺は台所に行き、いつもつけている前掛けを装備する。\\nそこまで料理は得意ではないが、それなりにできるはず。社長令嬢が食べていた食事と比べればお粗末なものができるだろう。\\nだが、何も食べないよりはましだ。\\n昨日作っておいた調理済みの料理を冷蔵庫から出し、皿に盛りつける。\\nあとはご飯と、汁物。汁物は作り置きが無いので適当に作る。\\n火の通りやすい野菜にワカメでいいか。\\nそして、最後に魔法をかける。おそらく誰しもが使った事のある魔法。そう、レンチンだ。\\n盛ったおかずに、ご飯をレンジに入れチンする。\\n汁物だけは今作ったので温かさそのまま。\\n――チン!\\nレンジから取り出した料理をトレイにおき、箸をそえ台所を後にする。\\n俺は雑誌を読んでる姫川の目の前に、適当に準備したご飯を置いていく。\\nまぁ、女の子ならこの位か? と適当によそったので、姫川からみたら多いのかもしれない。\\n「多かったり、まずかったら残してもいい。何も食べないよりましだろ?」\\n「なんで? どうして私なんかに?」\\n答えに困る。俺はどうして姫川にご飯を提供した?\\n俺に何の得がある? 見返りを期待しているのか? いや、違うな。\\n「はら減ってないのか? 減ってたら食えよ。何も食べていなかったんだろ?」\\n「いいの? 好き嫌い無いから全部食べてしまうわよ?」\\n「あぁ、俺もその方がいい。残されても困るからな」\\n姫川は箸を取り、ご飯を食べ始める。\\n流石令嬢。見てて美しく食べる。ただの適当飯なのに、令嬢が食べるとものすごい料理なのかと見間違ってしまう位だ。\\nまぁ、中身は俺が適当に作った料理なんだけどな。\\nしばらく沈黙の時間が流れる。その間、姫川は一言も話さず食事を続けている。\\nそして、箸を持ったまま動かなくなった。\\n「ん? どうした? まずかったか?」\\n姫川の頬を涙が流れ落ちるのが見えた。\\n「ち、違うの。誰かと一緒にご飯を食べる事、もう何年もなかったから......」\\n令嬢は令嬢で大変なんだなと思いつつ、正直俺の知らない世界の事を言われても困る。\\n「お手伝いさんが作ってくれたものを温めて、一人で食べたり、自分で作ったり......」\\n「母は私が小さい時に......。父も仕事が忙しくて......」\\nうーん、聞かない方が良かったのか、まぁ聞いてしまったものはしょうがない。\\nこれ以上は深く聞かないようにしよう。\\n「そっか。まぁ、メシ位はしっかり食べないとな。お代わりいるか?」\\n「大丈夫。ありがとう。これおいしいね」\\n瞼に涙を浮かべながらも、懸命に笑顔を作ろうとしている姫川が痛い。\\n俺まで心をチクチクされているようだ。俺はこういった精神攻撃にあまり強くない。\\nこれ以上は危険だ。俺の方がもらい涙してしまう。\\n「そいつは、ありがとさん」\\nその後、食事も終え改めて姫川の話を聞こうと仕切りなおす。\\n「で、『実は......』の続きは?」\\n「はい。父と最後に交わした言葉は『俺ははめられた。杏里は騙されるな。誰も信用するな!』そこで父との電話は切れました。私は報道で流れている事を信じていないの」\\nそれから、土曜の朝には会社の人事部を名乗るものがマンションにやってきて、もう住めなくなるからと追い出された。\\n日曜までにマンションを出るように言われ、手荷物もそこそこ駅前にやって来たらしい。\\n「親族とかいないのか?」\\n「父以外の親族はいません。頼りになる人も......」\\n社長とか令嬢とか言っても普段は見えていない部分が多いんだな。\\n何でもできて、見た目も良くて、周りにチヤホヤされて、全てがそろっていると思い込んでいた。\\nそれなりに苦労もしているようだ。\\n「とりあえず、今日は空き部屋に泊まっていけ。鍵もあるから、戸締りしっかりしろよ」\\n話もそこそこ、これ以上深追いしても俺が苦労しそうな気がしてきたので切り上げてしまった。\\n最後まで聞けばよかったのか? 途中で切り上げて良かったのか......。\\n部屋に姫川を案内し、カゴを渡す。\\n「とりあえずタオル一式。石鹸とかシャンプーとかは風呂場にあるけど、自分のを持っているならそれ使ってくれ。少なくとも女性用ではないと思うから」\\n姫川はキョトンとした感じで、俺の渡したカゴを受け取る。\\n続いて俺は部屋について簡単に説明をする。\\nロフト式のベッドはすぐに使う事ができ、ベッドの下にはクローゼット兼押入れがある事を伝える。\\nそして、押入れには布団が圧縮袋に入っているので寝る時に使ってほしい事と、クローゼットにはハンガーが数本ある事を伝える。\\n「多分探せばクッションとか枕とかあると思うから、適当に使ってくれ。他に必要なものがあったら声かけてくれ」\\n「じゃぁ、俺は風呂の準備でもしてくるから、適当な時間に風呂にでも入ってくれ」\\n姫川は部屋の中を見渡し、手に持っていた手荷物を床に置く。\\nそして、そのまま部屋の奥の方へに入っていったので、俺は扉を閉める。\\nこのまま何事も無ければいいんだがな。\\nあとは姫川が風呂に入っている間に......。\\nしばらくすると階段を下りてくる音が聞こえる。\\n『お風呂、入るわね』\\n扉越しに姫川の声が聞こえる。\\n「あぁ、俺以外誰もいないけど鍵かけろよ」\\n『今日はありがとう。私に声をかけてくれて......』\\nこうして姫川の足音が遠ざかって行く中、俺は自室のパソコンに再び向かった。',\n",
       " 'ユーザー: 「Action, Adventure, Comedy, Ecchi, Fantasy, Harem, Mature, Romance, Seinen, Supernatural」のジャンルが含まれるストーリーを書いて。\\nシステム: 第九話 防具で殴っちゃいけないとは誰もいっていない\\n霊山の麓村を出発してから四日後、俺らの一行は新たな町にたどり着いていた。レアルの談では規模はそこそこでなかなかに賑やかな活気がある。俺としては初めて町らしい町に訪れた。王城の城下町は竜に乗って上空からチラッとみただけだし、霊山の麓村は小さかったし。\\n町並みは俺が日頃に読んでいたマンガに出てくる中世西洋の色が濃い。家屋の素材は、麓村にあった木造中心ではなく煉瓦作りが殆ど。道を歩く市民の服装も、心なしか色とりどりに晴れやかだ。そういえば、現実世界(俺的な元の世界の意味)でのイタリアは未だに昔の町並みを色濃く残しているそうだ。\\n「人間ってのは、世界は違えど発展の仕方はどこも変わらないのかもしれないな」\\n予定としては、この町に数日間留まり、これまでの道程で消費した食料等の旅用品の補給をするつもりだ。できるならば、麓の村では入手できなかった便利グッズがあるならこちらも入手したい。これだけ大きい町なら他所との交易もあるだろうし、物資の流入も多い。\\n「馬の一つも手に入ればな」\\n「今の手持ちで買えるのか?」\\n「馬といってもピンキリだ。安い物なら何とかなるだろう。なければないで我慢するが、あるとないとでは旅路の疲労度が段違いだ」\\n確かに、馬に乗っている間は、俺たちは体を休めることができる。そうすれば、道程の間に襲いかかってくる魔獣やら盗賊や等と出会っても万全に近い態勢で迎撃できる。事実、この四日感で一度だけ寝る寸前に魔獣の群が襲いかかっていたが、心身共に疲労した状態でそれを迎え撃つのは中々に大変だった。ヒヤッとする場面は無かったが、休憩を挟みながらも一日歩き通した上での戦闘は辛い。\\n「そういえば、カンナは馬の扱いは?」\\n「こちとら都会のもやしっこだ。ガキの頃に遠足で言った牧場で、試乗したのが最後だ」\\n俺らはしばらくの拠点となる宿屋を探す。大きい町なので、宿屋には事欠かない。ただ、宿と一概にいってもランクは勿論あり、世間知らずの俺はそこら辺の判断はレアルにまかせっきりである。\\n「では、今日はここにするか」\\n彼女が選んだのは、三階建ての中々に立派な作りをした宿屋だった。看板に簡易な宿泊費が書いてあったが、これまで通り過ぎていた宿屋よりも若干割高だ。\\n「ちょいと高くねぇか?」\\n「だろうな。だが、一般には知られていないが我々は尋ね人だ。下手に安い宿屋は簡単に客の情報を漏らす。逆にこういった宿は、しっかりとした金払いがされれば客の個人情報の秘匿をしてくれる」\\n「安全をお金で買うんだな」\\n「その通り。臑に傷がある者は、むしろランクの高い宿屋を借りるのが常套手段で、我らとしては好都合。下手に金をケチるよりは、多少割高でも安全面を買うべきだ。さすがに、国家権力を振りかざされれば無理だろうが、騒ぎにしたくない以上それもしないだろう」\\n身に覚えのない臑の傷だ。小さな憤慨を飲み込み、俺たちは宿屋の扉を開けた。\\n受付との交渉の結果、俺らは一人部屋を一つずつ借りた。男女の振り分けだ。期間は二泊三日。やはり値段は少々高くついたが、レアルの言うとおりに安全面を考慮すれば妥当な出費。部屋の扉には鍵がついているし、金庫もある。\\nさらには部屋毎にお風呂まで。異性を気にせずに湯船に浸かれるのは大きい。\\nーーーー実は麓の村に滞在中、風呂上がりのレアルと一度出くわしてしまった時のこと。\\n普段の鎧姿ではなく、薄い肌着姿だったのだが、アレはヤバかった。水気でシットリした髪の具合や、上気した肌の色気もさることながら。実は彼女、婆ちゃん以上にスタイルが抜群だったのだ。頭の中にGとかHとかそこら辺の単語が頭の中に浮かんだ。あれだけの巨大質量をどうやって鎧の奥に納めているのか気になる。しかも本人はそこらに無頓着だからさらに困る。\\nエルフ耳でクールビューティーであの隠れ巨乳とか反則だろう。どんだけ属性多いんだよ。某オタクの聖地に放り込んだら、歓喜の阿鼻叫喚が巻き起こる。\\nそんなわけで、俺は村に滞在中、レアルといかに風呂で鉢合わせにならないかに細心の注意を払っていた。あれだけの肢体を生で見せられたら、若き思春期な俺の理性が持ちそうにない。その甲斐があり、俺のリビドーが迸るようなシチュエーションに陥る事態は起こらなかった。妙なところで暴発する危険性は増えたが。\\nあれだね。一見するとエロゲ的展開だが、案外心労が絶えないね。下手に異性間の問題が起こったら、その後の展開が気まずすぎる。\\n宿に荷物を置いた俺たちは、具体的な方針を話し合うため、昼食もかねて外にでる。この宿も朝昼晩に食事はでているが、麓の村の時のように食堂は常時開放でないらしい。俺たちが宿に着いたのが昼を少し過ぎた頃だったので、荷物を整理している間に時間が過ぎてしまったのだ。\\n昼食どころに選んだのは、こじゃれたオープンテラスの店だ。売りはソースが掛かったパスタで、味はミートスパゲティに近い。\\n「とりあえず、私の装備の方を鍛冶屋の方に預けてこよう。防具はともかく、剣は一度奪われてから今の今までろくな整備が出来なかったからな。ついでに君に装備も見繕おう。さすがにその服のままではこの先何かと不安だ」\\n俺の現装備は、簡素な布の服に皮の胸当てと手甲、脚甲という出で立ちだ。ナマクラは防げても、少し切れ味のよい刃物相手だと多分紙装甲同然である。一応、防御手段は精霊様こと婆ちゃんに教わったが、咄嗟にそれが出来るかと問われれば無理である。素の防御力アップは必須だ。\\n「村にいた一週間で君の訓練を行い、その上でこの町に着くまでの四日間を省みるに、筋は悪くないように見える。動き易さを重視した装備がいいだろう。聞いてはいなかったが、君は何か武術をたしなんでいたのかい?」\\n「ガキの頃からの幼なじみがトラブルメーカーだったのと、別の友人が空手有段者だったから、自然とな」\\n「カラテ?」\\n「素手の武道の一つだ。実力者になると瓦............じゃわからねぇか。数枚重ねた薄いレンガを拳で叩き割れる」\\n美咲の場合、素手で五枚だが、踵落としだと二十枚ぐらいいけると自慢げに豪語していた。女性として自慢していい部分なのかと思う。\\n「ほぉ、それは興味深いな。いつか手合わせをお願いしてみたいものだ」\\n好戦的にレアルが口の端をつり上げた。\\n「所詮は平和な世界の出来事だ。全身鎧姿の兵士を一薙で吹き飛ばせるレアルの相手にはならないだろうさ。それに、前提条件として無理がある」\\n何しろ此方と彼方は次元を隔てているのだ。\\n「ま、そんな友人とじゃれ合ってると、自然と体も動くのよ。さり気ないスキンシップが結構痛いんだ。避けるか防がないと生傷が絶えない」\\n主に有月の生傷が絶えなかったが、あれはほぼ自業自得だ。\\n「お陰で、急な荒事が起こっても何とか躯が動く程度の度胸が付いたんだから、人生って何が役に立つか分からないよな」\\n「そうだな。少なくとも今だけはそのトラブルメーカーとカラテの友人に感謝しておくべきだろう」\\nーー後に。\\nそのトラブルメーカーが諸悪の根源であると発覚するのだが、今はそれに気が付く由もなかった。\\n装備屋に行く前に、手持ちの金銭が心許ないので先に質屋に行くことになった。無論、売買するのは城から拝借した貴金属類だ。とりあえず、何かしらの刻印や豪華な作りをしているのは除外し、作りの簡単な宝石や装飾を優先的に売り払う。前者だと、名のある細工師が手がけている可能性があり、それが市場に出回ると逆算して俺たちの存在が発覚しかねないからだ。\\nそういえば、俺たちがどうしてこんな財宝類を所持しているかは説明したが、どうやってそれらが保管してある宝物庫に侵入できたかは説明不備だったな。普通、ああいった貴重品を納めた部屋は、厳重な扉と鍵によって施錠されており、\\nレアル曰く。\\n「あの扉には鍵穴が無かったからな。鍵は特殊な魔導具だったのだろう。登録された魔力の持ち主でなければ開かない仕組みだ。それ以外の者が無理にこじ開けようとすれば、何らかの防犯装置が作動するのが大半だ」\\nや、普通に開いちゃいましたけど。\\n「あの魔導具の扉を作った技師も、さすがに『魔力を持たない者』までは想定していなかったのだろう。どんな人間だろうと、通常は大なり小なりに魔力を秘めているからな。君の存在は正しく設計の想定外だったのだろうな。もしかしたら、君にとってはむしろ普通の錠前の方が防犯上は効果的だ」\\nハイテク(?)の弊害はどこの世にもあるらしい。\\nそんなこんなで補足説明を挟んでパクった財宝類を換金したのだが、なかなかのお値段になった。\\nこの世界の金銭は紙幣ではなく硬貨で循環している。上から金、銀、銅、鉄と段階がある。一番高い金貨が十万円。その銀貨が一万円。さらに下がって千円、百円と多分基準はこんな感じ。同じ種類の貨幣が十枚集まって一つランク上の貨幣になる。道端の露天で売ってるパンが鉄貨一枚なのが判断要素だ。\\n今回換金したのは金貨にして三十枚ほど。日本円にして三百万円程か。日本サラリーマンの平均年収並。この世界の一般家庭の年収が金貨十〜十五らしいので、どちらにせよかなりの大金だ。さらに、今回は全て換金したのではなく、全体の四分の一を処分したにとどまる。つまり、最大でこの三倍、軽く見積もっても二倍近くの貯蓄がある計算になる。\\n何で一気に全部を処分しなかったのかというと、これだけ大量の貴金属類の全て質屋に持って行くと、何かしらの疑いを掛けられる可能性があるからだ。\\n金貨三十枚の入った皮袋の重さに恐れを抱きつつ、俺たちは今度こそ装備屋にへと向かった。\\n「さてカンナ、とりあえず剣の方は整備の方に預けたので君の物見繕おうか............どうした?」\\n俺は、装備屋の壁に立て掛けてある剣の値段に言葉を失っていた。\\nロングソードだと思わしき両刃の長剣のお値段、金貨三枚。日本円換算で三十万円である。隣のショートソードも金貨一枚。その下にある更に短い剣も、銀貨五枚。\\n「............武器ってこんなに高いのか」\\nRPGに出てくる序盤の武器は、大体千円(でいいのか?)かそこらで購入できる。リアルとゲームをごっちゃにするのは良くないが、どこかしらで似通った意識を持っていたのだと、今更ながらに気が付く。\\n「使い手が命を預ける代物だからな。作る側としても少なからずの労力をそそぎ込んでる。実戦に耐えうる物となると、大体がそこに置いてあるものと同等の価格だ。それ以下になると、素材や作りの悪い粗悪品の可能性を疑った方がいい」\\n「古代の遺跡からの発掘品や、魔獣の死骸から取れる希少性のある素材は、高い値で売れるからな。命の危険はあっても、それに見合う報酬がある。だからこそ、武器の供給と需要のバランスが保たれているのだ」\\nそりゃそうだ。買う相手がいるから売る側も存在できるのだ。これら武器の値段も、冒険者たちの収入からして無理のない設定なのだ。\\n「参考までに聞くけど、レアルの剣は幾らぐらいになるんだ?」\\n興味本位に問いかけると、レアルは顎に手を当てて考え込む。\\n「そうだな............最低価格でもーー金貨百枚は行くか?」\\n「なにそれめっちゃ高いな。あんたそんなもん振り回してたのか」\\n背中に金の延べ棒を背負っているのと同じではないか。しかも最低価格ときましたか。とりあえず家が建つな。\\nだが、あれはあれで祖父の代の更に上の代から我が家に伝わる家宝だからな。自画自賛な点もあるだろうが、あれほどの頑強さと切れ味を併せ持った剣はそう多くは無いだろう。ただ、祖父も父もあれは家宝ではあるがやはり武器には変わりないから、常に戦場に携えていったよ。正確に値段の鑑定は行ったことはない。これは私の推測だ」\\n「そんなん預けて大丈夫なのかよ」\\n「言いたいことは分かる。もし万が一、装備屋が預かった武器が紛失して見ろ、その店の評判は一瞬で崩れて廃業一直線だ。加えて、武器を預けた冒険者の恨みも買う。それが名の知れた冒険者の得物なら尚更だ」\\nファンタジーな世界だが、商売に関しては非常にリアルである、と今回の装備や訪問で学ぶのであった。信頼関係って大事だよね。\\n今回のお買い物で、俺の装備は鉄製の胸当てと手甲、足甲に変更された。俺の希望とレアルの見立てに沿い、動きを阻害せずに防御力を底上げするオーダー。手甲は拳を握ると指を覆ってくれる風な設計の物を選んだ。剣を握るならその部分は邪魔になりやすいが、武器を持たずに相手を直接殴るのならば問題はない。むしろ手足の防御力と重量が増したので攻撃力もアップである。レアルの方は装備は殆ど変わらず、剣の整備だけに留まる。ただし武器の整備費が結構掛かったようだ。\\n装備を調えた経費の合計は金貨八枚。およそ八十万円。必要経費で妥当なお値段なのだろうが、一小庶民の俺にとってはやはり大金に思えた。\\nこの金銭感覚が現実世界に戻ったときに引きずらなければいいが、と小さな心配事が増えたのだった。',\n",
       " 'ユーザー: 「Adventure, Fantasy, Shoujo, Slice of Life」のジャンルが含まれるストーリーを書いて。\\nシステム: 「レルンダ、遺跡へはまず、少人数で向かってもらう事にしようと思います。まずは調査を行って、安全性が確認できてから、レルンダが遺跡へ行くことにしましょう。私も調査に行こうと思います」\\n「ランさんも?」\\n「ええ。忘れ去られた遥か昔の国の遺跡というのは、興味深いものですから。それに研究者である私が行くからこその発見もあるのではないかと思います。私ももっとこの村のために力になりたいですから」\\n遺跡へ向かう計画――まずは、調査に何人かが向かうことになっているようだ。\\n私やガイアスはまだ子供だから、先に大人たちから向かうんだって。ガイアスは私よりも年上だから、もうすぐ成人はするだろうけど、この村の中ではまだ子供に分類されるのだ。\\nそれにしてもランさんは遺跡に向かう不安などよりも、遺跡でどれだけ新しいものが見つかるだろうかという興奮でいっぱいのようだ。\\n本当にランさんは、新しいことが大好きだなとそんな風に思う。\\n「ランさん、それは危険ではないの? 大丈夫?」\\n「心配してくれているのですね。ありがとうございます。大丈夫ですよ。ニルシさんたちが訪れた時には危険がなかったということですから、危険な目に陥る可能性は少ないでしょう。あとですね、レルンダ、もしレルンダが許可を出してくださるのならレルンダが契約している誰かを一緒に遺跡の調査に向かわせることは可能でしょうか?」\\n「うん。もちろん。大丈夫」\\nグリフォンやシーフォ、フレネたちが一緒に行った方がきっと、皆が危険に遭遇する可能性もぐっと下がるだろう。\\nドアネーアはまだ小さいから駄目だけど、皆に言ったらランさんたちと一緒にいてくれるだろう。\\n「ありがとうございます。レルンダ。\\n危険性がないことが確認を出来たら今度はレルンダも一緒に遺跡に向かいましょうね」\\n「うん」\\nランさんは私が頷けば、優しい笑みを浮かべてくれた。\\nああ、でもランさんが遺跡に調査に行ってしまったら少しだけ寂しいなと思う。\\nランさんと一緒の家で過ごしているから、しばらく寂しいなという気持ちで一杯になってしまうかもしれない。\\nでもランさんはこれだけ遺跡に行くのを楽しみにしているし、遺跡に行くことでこの村にも良い影響を与えられるかもしれないから、寂しいではなく、頑張ってねって送り出したい。\\nそう思っているのだけど、表情に出てしまっていたらしい。\\n「レルンダ、どうした」\\n「ニルシさん......」\\n私が広場でシーフォと一緒に歩いていたら、前から歩いてきたニルシさんにそんなことを問いかけられた。\\n顔に出ていたのだろうか?\\n「ニルシさんも、遺跡行く?」\\n「遺跡への調査か? そうだな。行くぞ。俺は一度行っているから案内も出来るしな」\\n「そっか」\\n折角帰ってきたニルシさんも、また遺跡に行ってしまうのかと思うとやっぱり少しだけ寂しかった。\\n「なんだ、寂しいのか?」\\nニルシさんはそう問いかけたかと思えば、少しだけ乱暴に私の頭をわしゃわしゃと撫でる。髪がぐちゃぐちゃになる。\\n「遺跡は此処からそこまで遠くはない。この前のように長期間、此処を離れるわけでもないから、そんなに寂しがらなくていいぞ」\\n「......うん」\\n「まぁ、ちゃんと無事に帰ってくるつもりだから、心配はするな。ランのこともちゃんと、暴走しないように見とくから」\\n「うん。ちゃんと無事に戻って来て欲しい。でも確かに、ランさん、暴走しそう。遺跡とか大好きだから」\\n「だよなぁ......。好奇心旺盛すぎて、あいつは子供かって感じだよな」\\n「ランさん、大人だよ。知りたいこと沢山な人だけど、私、憧れる」\\n「......いや、レルンダはああならない方がいいと思うぞ? 目指すならこう他の大人を――」\\nニルシさんは微妙な顔をしながら口にする。その最中に「何を言っているのですか」といつの間にか近くにいたランさんがニルシさんを軽くにらんでいた。\\n「何をって、ランみたいに子供みたいにはしゃぐ大人が増えても大変だろう」\\n「全く私は確かに好きな事だと夢中にはなることは自覚しておりますが、そこまで言われるほどではありませんよ。レルンダ、ニルシさんのような大人にもなってはなりませんよ? レルンダは今の素直で可愛いレルンダのまま大人になってくださいね?」\\n「......お前、ひどい言い草だな?」\\n「先に言ったのはそっちでしょ?」\\nランさんとニルシさんは口喧嘩を始めてしまった。でも二人とも何処か楽しそうで、不思議だなと思う。\\n仲が悪いというわけでもないから、これが二人の交友なんだなとも思う。\\n「あ、そうですわ。レルンダ」\\n「どうしたの?」\\n急に口喧嘩をやめたかと思えば、ランさんはこちらを見る。\\n「遺跡に私たちが向かう前にですね。精霊樹の復活のお祝いをやろうと思うのですよ。私たちは精霊の姿が見えないので、レルンダは精霊たちに話を通しておいてほしいのです」\\n「そっか。もう一年経ったの」\\n「ええ。そうですよ。精霊樹が復活してもう一年ですからね。またお祝いをしたいのですよ」\\n「うん。言っておく」\\n私はランさんの言葉に頷くのだった。\\n(神子な少女は女史たちが遺跡に行く話を聞いて寂しさを感じる)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ds['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d75919",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = formatted_ds.train_test_split(test_size=0.1, seed=10) #seeded so the split is not random\n",
    "\n",
    "train_set = split_ds['train']\n",
    "eval_set = split_ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101bc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109c2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9b9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(65536, 2816)\n",
      "    (emb_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (query_key_value): Linear4bit(in_features=2816, out_features=8448, bias=True)\n",
      "          (dense): Linear4bit(in_features=2816, out_features=2816, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=2816, out_features=11264, bias=True)\n",
      "          (dense_4h_to_h): Linear4bit(in_features=11264, out_features=2816, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=2816, out_features=65536, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4b09d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='477' max='477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [477/477 8:16:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>3.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>3.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>3.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.275500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=477, training_loss=3.867783244550853, metrics={'train_runtime': 29871.4959, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.016, 'total_flos': 2.1252751603623936e+16, 'train_loss': 3.867783244550853})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-fine-tuned-model\",\n",
    "    num_train_epochs=1,              # Specifically setting 1 epoch\n",
    "    per_device_train_batch_size=2,   # Adjust based on GPU VRAM\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"            # Save after the epoch is done\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    return example['text'] # The trainer expects a list of strings\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_set,      \n",
    "    processing_class=tokenizer,          \n",
    "    args=training_args,           \n",
    "    peft_config=lora_config,      \n",
    "    formatting_func=formatting_prompts_func, \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a6336",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af668212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Cap at 512 or 1024 tokens next time instead of whole chapters cause this took way too long \n",
    "from peft import PeftModel\n",
    "\n",
    "lora_checkpoint_path = \"./lora-fine-tuned-model/checkpoint-477\"\n",
    "model = PeftModel.from_pretrained(model, lora_checkpoint_path)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「Shounen, Adventure」のジャンルが含まれるストーリーを書いて。 また、お酒のラベルも書いて。 これまた、とってもお上手。 しかも、ちゃんと、お酒の味もわかって、お上手。 で、このあと、「あのラベルってさ」 って、お母さまに見せたりしたら、 「えーーー! あの子、今年、お酒のラベルまで、書いてたんだよー」 って、お母さまは、かなり、お気に召したよう。 この\n"
     ]
    }
   ],
   "source": [
    "# One prompt to test\n",
    "text = \"「Shounen, Adventure」のジャンルが含まれるストーリーを書いて。\"\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d32abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting the Story ---\n",
      "「Shounen, Adventure」のジャンルが含まれるストーリーを書いて。「(今日は)自分の投球をしっかりできたし、先制点を取ることができたのでよかったです」\n",
      "「試合前に、みんなから『絶対抑えろ』って言われてました。でも10回で2アウトになって、1対0だったので、あまり気持ち的には変わらないんですけどね(笑)」\n",
      "「(逆転された5回裏に)あそこから粘れたというのは、自分にも自信になりますけど、相手のピッチャーにもすごく大きなものがあったと思います。そこを取り返してやるという強い思いが伝わってきてたので、それが最後の1点がこっちに来たんだろうなと。やっぱり悔しいですよね」\n",
      "「これからもっと厳しい練習になっていくと思うんですけれども、そういう意味では、自分がこうやって経験させてもらっていることに感謝しながら、とにかく頑張っていきたいと思います」\n",
      "「今年は結果が出なくて、悔しくて、この1年何をやってきたのかと反省するところもあったんですけれども、本当にいろんな人が支えてくれて、一緒に戦ってきてくれたことがすごく大きかったと思います。そういう方々に恩返しできるように頑張りたいとおもいます」\n",
      "「今日みたいな戦い方をしてくれると、これからもやりやすいんじゃないかと思いましたね。なかなか、ああいう戦い方は見たことがないんで、楽しかったですね。それくらいしか言えないです」\n",
      "--- Model reached end of story. ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_length = 2000\n",
    "max_new_tokens_per_loop = 512 \n",
    "story = \"「Shounen, Adventure」のジャンルが含まれるストーリーを書いて。\"\n",
    "\n",
    "# Encode the initial prompt\n",
    "input_ids = tokenizer.encode(story, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"--- Starting the Story ---\")\n",
    "print(story, end=\"\")\n",
    "\n",
    "while input_ids.shape[1] < target_length:\n",
    "    # Generate the next segment\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens_per_loop,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    new_tokens = output_sequences[:, input_ids.shape[1]:]\n",
    "    \n",
    "    new_text = tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "    input_ids = output_sequences\n",
    "\n",
    "    # Safety break if the model generates an eos token\n",
    "    if tokenizer.eos_token_id in new_tokens:\n",
    "        print(\"\\n--- Model reached end of story. ---\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e97dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
